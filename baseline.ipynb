{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a321b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the important files\n",
    "from prepare_data import *\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921ea8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae13c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_address=\"food-101/train.csv\"\n",
    "test_address=\"food-101/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6761ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val,test=get_dataloaders(training_address, test_address, args=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ba3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(baseline, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n",
    "        self.BN1=nn.BatchNorm2d(64)\n",
    "        self.conv2= nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.BN2=nn.BatchNorm2d(128)\n",
    "        self.conv3= nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.BN3=nn.BatchNorm2d(128)\n",
    "        self.conv4= nn.Conv2d(128, 128, kernel_size=3,stride=2)\n",
    "        self.BN4=nn.BatchNorm2d(128)\n",
    "        self.avg_pool=nn.AdaptiveAvgPool2d(1)\n",
    "        # Compute the input units using the formula given in discussion session 128*63*63\n",
    "        self.fc1=nn.Linear(128, 128)\n",
    "        self.drop_out=nn.Dropout()\n",
    "        self.fc2=nn.Linear(128,20)\n",
    "    def forward(self, x):\n",
    "        # layer1\n",
    "        x=self.conv1(x)\n",
    "        x=self.BN1(x)\n",
    "        x=F.relu(x)\n",
    "        # Layer2\n",
    "        x=self.conv2(x)\n",
    "        x=self.BN2(x)\n",
    "        x=F.relu(x)\n",
    "        # Layer3\n",
    "        x=self.conv3(x)\n",
    "        x=self.BN3(x)\n",
    "        x=F.relu(x)\n",
    "        x=F.max_pool2d(x,3)\n",
    "        # Layer4\n",
    "        x=self.conv4(x)\n",
    "        x=self.BN4(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.avg_pool(x)\n",
    "        # fully connected layer 1\n",
    "        x=x.view(-1,128) # faltten the input\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(self.drop_out(x))\n",
    "        # fully connected layer 2\n",
    "        x=self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3291e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "def prepare_model(device, model):\n",
    "    # load model, criterion, optimizer, and learning rate scheduler\n",
    "    \n",
    "    # Create an empty model and move it to the designated computing device\n",
    "    model=model.to(device)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_single_model(model, criterion, optimizer, scheduler, device, dataloaders,epoch):\n",
    "    \n",
    "    # prepare the model\n",
    "    model=prepare_model(device,model)\n",
    "    \n",
    "    # store the results of our training\n",
    "    training_losses=[]\n",
    "    training_accs=[]\n",
    "    val_losses=[]\n",
    "    val_accs=[]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        # train the model in each epoch\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(dataloaders[0]):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad() # clear the old gradients \n",
    "            output = model(data) # compute outputs of the fc layer \n",
    "            loss = criterion(output, target) \n",
    "            loss.backward() # compute gradient for every variables with requires_grad=True\n",
    "            optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print(\"finish a epoch of training\")\n",
    "            \n",
    "        # test the model with the training data:\n",
    "        model.eval() # sets model in evaluation (inference) mode. Q3. Why? \n",
    "        training_loss = 0 \n",
    "        correct = 0\n",
    "        with torch.no_grad(): # stop storing gradients for the variables\n",
    "            for data, target in dataloaders[0]:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                training_loss+=criterion(output, target)\n",
    "                pred = output.argmax(dim=1, keepdim=True) # get the index of maximum fc output. Q4. Why?\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        training_loss /= len(dataloaders[0].dataset)\n",
    "        training_losses.append(training_loss)\n",
    "        training_acc=correct / len(dataloaders[0].dataset)\n",
    "        training_accs.append(training_acc)\n",
    "        \n",
    "        \n",
    "        # test the model with validation data in each epoch\n",
    "        model.eval() # sets model in evaluation (inference) mode. Q3. Why? \n",
    "        val_loss = 0 \n",
    "        correct = 0\n",
    "        with torch.no_grad(): # stop storing gradients for the variables\n",
    "            for data, target in dataloaders[1]:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss+=criterion(output, target)\n",
    "                pred = output.argmax(dim=1, keepdim=True) # get the index of maximum fc output. Q4. Why?\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        val_loss /= len(dataloaders[1].dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc=correct / len(dataloaders[1].dataset)\n",
    "        val_accs.append(val_acc)\n",
    "        print(\"epoch_num: \"+str(i))\n",
    "        print(\"train_loss: \"+str(training_loss))\n",
    "        print(\"val_loss: \"+str(val_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), \"food-101/state_dict_model.pt\")\n",
    "    return model,training_losses,training_accs,val_losses,val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e054f653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baseline(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (BN2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (BN3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (BN4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=baseline()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "def Xavier_Initialization(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.normal_(m.bias.data, mean=0.0, std=1.0)\n",
    "model.apply(Xavier_Initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b08530",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[train,val,test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a78804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish a epoch of training\n",
      "epoch_num: 0\n",
      "train_loss: tensor(0.0857, device='cuda:0')\n",
      "val_loss: tensor(0.0514, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 1\n",
      "train_loss: tensor(0.0837, device='cuda:0')\n",
      "val_loss: tensor(0.0503, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 2\n",
      "train_loss: tensor(0.0804, device='cuda:0')\n",
      "val_loss: tensor(0.0482, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 3\n",
      "train_loss: tensor(0.0793, device='cuda:0')\n",
      "val_loss: tensor(0.0478, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 4\n",
      "train_loss: tensor(0.0776, device='cuda:0')\n",
      "val_loss: tensor(0.0470, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 5\n",
      "train_loss: tensor(0.0774, device='cuda:0')\n",
      "val_loss: tensor(0.0467, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 6\n",
      "train_loss: tensor(0.0749, device='cuda:0')\n",
      "val_loss: tensor(0.0452, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 7\n",
      "train_loss: tensor(0.0741, device='cuda:0')\n",
      "val_loss: tensor(0.0450, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 8\n",
      "train_loss: tensor(0.0750, device='cuda:0')\n",
      "val_loss: tensor(0.0453, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 9\n",
      "train_loss: tensor(0.0715, device='cuda:0')\n",
      "val_loss: tensor(0.0433, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 10\n",
      "train_loss: tensor(0.0714, device='cuda:0')\n",
      "val_loss: tensor(0.0433, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 11\n",
      "train_loss: tensor(0.0712, device='cuda:0')\n",
      "val_loss: tensor(0.0431, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 12\n",
      "train_loss: tensor(0.0691, device='cuda:0')\n",
      "val_loss: tensor(0.0421, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 13\n",
      "train_loss: tensor(0.0695, device='cuda:0')\n",
      "val_loss: tensor(0.0424, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 14\n",
      "train_loss: tensor(0.0679, device='cuda:0')\n",
      "val_loss: tensor(0.0414, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 15\n",
      "train_loss: tensor(0.0695, device='cuda:0')\n",
      "val_loss: tensor(0.0421, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 16\n",
      "train_loss: tensor(0.0678, device='cuda:0')\n",
      "val_loss: tensor(0.0412, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 17\n",
      "train_loss: tensor(0.0653, device='cuda:0')\n",
      "val_loss: tensor(0.0402, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 18\n",
      "train_loss: tensor(0.0653, device='cuda:0')\n",
      "val_loss: tensor(0.0399, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 19\n",
      "train_loss: tensor(0.0648, device='cuda:0')\n",
      "val_loss: tensor(0.0399, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 20\n",
      "train_loss: tensor(0.0624, device='cuda:0')\n",
      "val_loss: tensor(0.0383, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 21\n",
      "train_loss: tensor(0.0614, device='cuda:0')\n",
      "val_loss: tensor(0.0378, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 22\n",
      "train_loss: tensor(0.0650, device='cuda:0')\n",
      "val_loss: tensor(0.0402, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 23\n",
      "train_loss: tensor(0.0613, device='cuda:0')\n",
      "val_loss: tensor(0.0380, device='cuda:0')\n",
      "finish a epoch of training\n",
      "epoch_num: 24\n",
      "train_loss: tensor(0.0607, device='cuda:0')\n",
      "val_loss: tensor(0.0377, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(baseline(\n",
       "   (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "   (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "   (BN2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "   (BN3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "   (BN4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "   (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (drop_out): Dropout(p=0.5, inplace=False)\n",
       "   (fc2): Linear(in_features=128, out_features=20, bias=True)\n",
       " ),\n",
       " [tensor(0.0857, device='cuda:0'),\n",
       "  tensor(0.0837, device='cuda:0'),\n",
       "  tensor(0.0804, device='cuda:0'),\n",
       "  tensor(0.0793, device='cuda:0'),\n",
       "  tensor(0.0776, device='cuda:0'),\n",
       "  tensor(0.0774, device='cuda:0'),\n",
       "  tensor(0.0749, device='cuda:0'),\n",
       "  tensor(0.0741, device='cuda:0'),\n",
       "  tensor(0.0750, device='cuda:0'),\n",
       "  tensor(0.0715, device='cuda:0'),\n",
       "  tensor(0.0714, device='cuda:0'),\n",
       "  tensor(0.0712, device='cuda:0'),\n",
       "  tensor(0.0691, device='cuda:0'),\n",
       "  tensor(0.0695, device='cuda:0'),\n",
       "  tensor(0.0679, device='cuda:0'),\n",
       "  tensor(0.0695, device='cuda:0'),\n",
       "  tensor(0.0678, device='cuda:0'),\n",
       "  tensor(0.0653, device='cuda:0'),\n",
       "  tensor(0.0653, device='cuda:0'),\n",
       "  tensor(0.0648, device='cuda:0'),\n",
       "  tensor(0.0624, device='cuda:0'),\n",
       "  tensor(0.0614, device='cuda:0'),\n",
       "  tensor(0.0650, device='cuda:0'),\n",
       "  tensor(0.0613, device='cuda:0'),\n",
       "  tensor(0.0607, device='cuda:0')],\n",
       " [0.21216666666666667,\n",
       "  0.2325,\n",
       "  0.26766666666666666,\n",
       "  0.2648333333333333,\n",
       "  0.30091666666666667,\n",
       "  0.29791666666666666,\n",
       "  0.3268333333333333,\n",
       "  0.32808333333333334,\n",
       "  0.31325,\n",
       "  0.3531666666666667,\n",
       "  0.35091666666666665,\n",
       "  0.34858333333333336,\n",
       "  0.37325,\n",
       "  0.3695833333333333,\n",
       "  0.3889166666666667,\n",
       "  0.3625833333333333,\n",
       "  0.37025,\n",
       "  0.403,\n",
       "  0.4056666666666667,\n",
       "  0.4156666666666667,\n",
       "  0.43116666666666664,\n",
       "  0.43933333333333335,\n",
       "  0.40091666666666664,\n",
       "  0.43616666666666665,\n",
       "  0.448],\n",
       " [tensor(0.0514, device='cuda:0'),\n",
       "  tensor(0.0503, device='cuda:0'),\n",
       "  tensor(0.0482, device='cuda:0'),\n",
       "  tensor(0.0478, device='cuda:0'),\n",
       "  tensor(0.0470, device='cuda:0'),\n",
       "  tensor(0.0467, device='cuda:0'),\n",
       "  tensor(0.0452, device='cuda:0'),\n",
       "  tensor(0.0450, device='cuda:0'),\n",
       "  tensor(0.0453, device='cuda:0'),\n",
       "  tensor(0.0433, device='cuda:0'),\n",
       "  tensor(0.0433, device='cuda:0'),\n",
       "  tensor(0.0431, device='cuda:0'),\n",
       "  tensor(0.0421, device='cuda:0'),\n",
       "  tensor(0.0424, device='cuda:0'),\n",
       "  tensor(0.0414, device='cuda:0'),\n",
       "  tensor(0.0421, device='cuda:0'),\n",
       "  tensor(0.0412, device='cuda:0'),\n",
       "  tensor(0.0402, device='cuda:0'),\n",
       "  tensor(0.0399, device='cuda:0'),\n",
       "  tensor(0.0399, device='cuda:0'),\n",
       "  tensor(0.0383, device='cuda:0'),\n",
       "  tensor(0.0378, device='cuda:0'),\n",
       "  tensor(0.0402, device='cuda:0'),\n",
       "  tensor(0.0380, device='cuda:0'),\n",
       "  tensor(0.0377, device='cuda:0')],\n",
       " [0.212,\n",
       "  0.22433333333333333,\n",
       "  0.25866666666666666,\n",
       "  0.259,\n",
       "  0.2793333333333333,\n",
       "  0.27666666666666667,\n",
       "  0.30966666666666665,\n",
       "  0.30766666666666664,\n",
       "  0.3006666666666667,\n",
       "  0.33666666666666667,\n",
       "  0.33666666666666667,\n",
       "  0.334,\n",
       "  0.36133333333333334,\n",
       "  0.3433333333333333,\n",
       "  0.36533333333333334,\n",
       "  0.3566666666666667,\n",
       "  0.35533333333333333,\n",
       "  0.37433333333333335,\n",
       "  0.389,\n",
       "  0.3903333333333333,\n",
       "  0.4063333333333333,\n",
       "  0.4146666666666667,\n",
       "  0.38233333333333336,\n",
       "  0.41633333333333333,\n",
       "  0.41633333333333333])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_single_model(model, criterion, optimizer, scheduler=None, device=\"cuda:0\", dataloaders=a,epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bbfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
